{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_relevancy_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, and a score rubric representing evaluation criteria are given. \n",
    "       1. You are provided with evaluation task with the help of a query with response and context.\n",
    "       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n",
    "       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n",
    "       4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)â€ \n",
    "       5. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "        ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n",
    "\n",
    "        ###Query and Response: {query_str} \n",
    "\n",
    "        ###Context: {context_str}\n",
    "            \n",
    "        ###Score Rubrics: \n",
    "        Score YES: If the response for the query is in line with the context information provided.\n",
    "        Score NO: If the response for the query is not in line with the context information provided.\n",
    "    \n",
    "        ###Feedback: \"\"\"\n",
    "\n",
    "prometheus_relevancy_refine_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query with response, context, an existing answer, and a score rubric representing a evaluation criteria are given. \n",
    "   1. You are provided with evaluation task with the help of a query with response and context and an existing answer.\n",
    "   2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n",
    "   3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n",
    "   4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)\" \n",
    "   5. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "   ###The instruction to evaluate: Your task is to evaluate if the response for the query is in line with the context information provided.\n",
    "\n",
    "   ###Query and Response: {query_str} \n",
    "\n",
    "   ###Context: {context_str}\n",
    "            \n",
    "   ###Score Rubrics: \n",
    "   Score YES: If the existing answer is already YES or If the response for the query is in line with the context information provided.\n",
    "   Score NO: If the existing answer is NO and If the response for the query is in line with the context information provided.\n",
    "    \n",
    "   ###Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.evaluation import RelevancyEvaluator\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Any, List, Dict\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "\n",
    "class GPT4RelevancyEvaluator:\n",
    "    \"\"\"Class for GPT-4 Relevancy Evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_template: str, refine_template: str):\n",
    "        # Initialize GPT-4 model and relevancy evaluator\n",
    "        gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
    "        \n",
    "        \n",
    "        HF_TOKEN = \"\"\n",
    "        HF_ENDPOINT_URL = (\n",
    "            \"\"\n",
    "        )\n",
    "\n",
    "        prometheus_llm = HuggingFaceInferenceAPI(\n",
    "        model_name=HF_ENDPOINT_URL,\n",
    "        token=HF_TOKEN,\n",
    "        temperature=0.1,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=40,\n",
    "        repetition_penalty=1.1,\n",
    "        )\n",
    "        \n",
    "        Settings.llm = prometheus_llm\n",
    "        \n",
    "        self.relevancy_evaluator = RelevancyEvaluator(\n",
    "            eval_template=eval_template,\n",
    "            refine_template=refine_template,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def evaluate_sources(self, query_str: str, response_vector: Any) -> List[Any]:\n",
    "        \"\"\"Evaluate relevancy for all source nodes in the response vector and return only 'Pass' nodes.\"\"\"\n",
    "        eval_source_result_full = [\n",
    "            self.relevancy_evaluator.evaluate(\n",
    "                query=query_str,\n",
    "                response=response_vector[\"query_result\"],\n",
    "                contexts=[source_node],\n",
    "            )\n",
    "            for source_node in response_vector[\"source_node\"]\n",
    "        ]\n",
    "\n",
    "        # Filter and return only the original source nodes where the evaluation passed\n",
    "        pass_nodes = [\n",
    "            source_node for result, source_node in zip(eval_source_result_full, response_vector[\"source_node\"]) if result.passing\n",
    "        ]\n",
    "        \n",
    "        return pass_nodes, len(pass_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import Any, List, Tuple\n",
    "from llama_index.core.tools.types import AsyncBaseTool\n",
    "from llama_index.core.workflow import Workflow, Context, Event, StartEvent, StopEvent, step\n",
    "import os\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "class FunctionCallEvent(Event):\n",
    "    func_call: Tuple[str, str, str]  # Function name, raw inputs, output placeholder\n",
    "\n",
    "class ValidateFunctionCallEvent(Event):\n",
    "    input_data: List[Any]\n",
    "    output_placeholder: str\n",
    "    tool_output: Any\n",
    "\n",
    "class InitializeEvent(Event):\n",
    "    \"\"\"Event for initializing the workflow context.\"\"\"\n",
    "    pass\n",
    "\n",
    "class COAWorkFlow(Workflow):\n",
    "    def __init__(self, timeout: int, max_num_sources = 3):\n",
    "        super().__init__(timeout=timeout)\n",
    "        self.max_num_sources = max_num_sources\n",
    "        evaluator = GPT4RelevancyEvaluator(prometheus_relevancy_eval_prompt_template, prometheus_relevancy_refine_prompt_template)\n",
    "\n",
    "\n",
    "        self.validator = evaluator\n",
    "        \n",
    "    @step(pass_context=True)\n",
    "    async def initialize_step(self, ctx: Context, ev: StartEvent | InitializeEvent) -> InitializeEvent | FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Initialize the workflow context.\"\"\"\n",
    "        if isinstance(ev, StartEvent):\n",
    "            # Perform initial setup, e.g., load tools, validators, etc.\n",
    "            ctx.data[\"results\"] = {}\n",
    "            ctx.data[\"tools_by_name\"] = ev.tools_by_name\n",
    "            ctx.data[\"function_calls\"] = ev.func_calls\n",
    "            ctx.data[\"iteration\"] = 0\n",
    "            ctx.data[\"accumulated_sources\"] = 0\n",
    "\n",
    "            # Produce an InitializeEvent to start initialization\n",
    "            return InitializeEvent()\n",
    "\n",
    "        # After initialization, check if there are function calls to process\n",
    "        if ctx.data[\"function_calls\"]:\n",
    "            first_func_call = ctx.data[\"function_calls\"][0]\n",
    "            return FunctionCallEvent(func_call=first_func_call)\n",
    "        else:\n",
    "            return StopEvent({\"message\": \"No function calls to process.\", \"results\": ctx.data[\"results\"]})\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def function_call_step(self, ctx: Context, ev: FunctionCallEvent) -> ValidateFunctionCallEvent | StopEvent:\n",
    "        \"\"\"Execute a function call and prepare for validation.\"\"\"\n",
    "        func_name, raw_inputs, output_placeholder = ev.func_call\n",
    "        input_data = self._prepare_inputs(ctx, raw_inputs)\n",
    "\n",
    "        try:\n",
    "            tool_output = await ctx.data[\"tools_by_name\"][func_name].acall(*input_data)\n",
    "            \n",
    "            # print(f\"Executed {func_name} with inputs {input_data} -> {output_placeholder}: {tool_output} \\n\")\n",
    "        except Exception as e:\n",
    "            return self._handle_error(f\"Error in {func_name} with inputs {input_data}: {e}\")\n",
    "\n",
    "        \n",
    "        if self.validator:\n",
    "            return ValidateFunctionCallEvent(\n",
    "                input_data=input_data + [tool_output.raw_output],\n",
    "                output_placeholder=output_placeholder,\n",
    "                tool_output=tool_output\n",
    "            )\n",
    "\n",
    "\n",
    "        ctx.data[\"results\"][output_placeholder] = tool_output.content\n",
    "        return await self._move_to_next_function(ctx)\n",
    "\n",
    "    def _prepare_inputs(self, ctx: Context, raw_inputs: str) -> List[Any]:\n",
    "        \"\"\"Parse and prepare function inputs.\"\"\"\n",
    "        results = ctx.data[\"results\"]\n",
    "        input_data = []\n",
    "        for raw_input in raw_inputs.split(\",\"):\n",
    "            raw_input = raw_input.strip()\n",
    "            try:\n",
    "                input_data.append(int(results.get(raw_input, raw_input)))\n",
    "            except ValueError:\n",
    "                input_data.append(raw_input)  # Handle non-integer inputs gracefully\n",
    "        return input_data\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def validate_function_step(self, ctx: Context, ev: ValidateFunctionCallEvent) -> FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Validate function output and decide on the next step.\"\"\"\n",
    "        try:\n",
    "            validator_output = self.validator.evaluate_sources(*ev.input_data)\n",
    "            # print(f\"Validation result: {validator_output} | Tool output: {ev.tool_output}\")\n",
    "        except Exception as e:\n",
    "            return self._handle_error(f\"Validation error: {e}\")\n",
    "\n",
    "        if validator_output:\n",
    "            \n",
    "            source_nodes, length_nodes = validator_output\n",
    "            \n",
    "            if length_nodes > 0:\n",
    "                ctx.data[\"results\"][ev.output_placeholder] = str(source_nodes)\n",
    "                # Accumulate source if validation is successful\n",
    "                ctx.data[\"accumulated_sources\"] += length_nodes\n",
    "\n",
    "                # Check if the maximum number of sources has been accumulated\n",
    "                if ctx.data[\"accumulated_sources\"] >= self.max_num_sources:\n",
    "                    return StopEvent({\"message\": \"Maximum number of sources accumulated.\", \"results\": ctx.data[\"results\"]})\n",
    "\n",
    "            return await self._move_to_next_function(ctx)\n",
    "\n",
    "        return self._handle_error(\"Tool output does not match the expected validation result\")\n",
    "\n",
    "    async def _move_to_next_function(self, ctx: Context) -> FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Move to the next function call if available.\"\"\"\n",
    "        iteration = ctx.data[\"iteration\"]\n",
    "        function_calls = ctx.data[\"function_calls\"]\n",
    "\n",
    "        if iteration + 1 < len(function_calls):\n",
    "            ctx.data[\"iteration\"] += 1\n",
    "            next_func_call = function_calls[ctx.data[\"iteration\"]]\n",
    "            return FunctionCallEvent(func_call=next_func_call)\n",
    "\n",
    "        # End of the workflow, produce a StopEvent\n",
    "        return StopEvent({\"message\": \"All function calls processed.\", \"results\": ctx.data[\"results\"]})\n",
    "\n",
    "    def _handle_error(self, message: str) -> StopEvent:\n",
    "        \"\"\"Handle errors gracefully and provide feedback.\"\"\"\n",
    "        return StopEvent({\"message\": message, \"results\": {}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "from llama_index.core.tools import AsyncBaseTool, ToolOutput\n",
    "from llama_index.core.types import BaseOutputParser\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class ChainOfAbstractionParser(BaseOutputParser):\n",
    "    \"\"\"Chain of abstraction output parser.\"\"\"\n",
    "\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        \"\"\"Initialize the parser with verbosity and workflow setup.\"\"\"\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def parse(\n",
    "        self, solution: str, tools_by_name: Dict[str, AsyncBaseTool]\n",
    "    ) -> Tuple[str, List[ToolOutput]]:\n",
    "        \"\"\"Run the async parse method, handling running event loops.\"\"\"\n",
    "        if asyncio.get_event_loop().is_running():\n",
    "            # Use `await` if inside a running event loop (Jupyter Notebook case)\n",
    "            return asyncio.get_event_loop().run_until_complete(self.aparse(solution, tools_by_name))\n",
    "        else:\n",
    "            # Normal use case outside of Jupyter, use asyncio.run\n",
    "            return asyncio.run(self.aparse(solution, tools_by_name))\n",
    "\n",
    "    async def aparse(\n",
    "        self, solution: str, tools_by_name: Dict[str, AsyncBaseTool]\n",
    "    ) -> Tuple[str, List[ToolOutput]]:\n",
    "        \"\"\"Asynchronously parse the solution and execute the workflow.\"\"\"\n",
    "        # Extract function calls\n",
    "        func_calls = re.findall(r\"\\[FUNC (\\w+)\\((.*?)\\) = (\\w+)\\]\", solution)\n",
    "        print(\"THIS IS\", func_calls)\n",
    "        # Initialize the workflow\n",
    "        workflow = COAWorkFlow(timeout=60)\n",
    "        # Run the workflow\n",
    "        response = await workflow.run(timeout=60,tools_by_name=tools_by_name, func_calls=func_calls)\n",
    "        \n",
    "        results = response[\"results\"]\n",
    "        # print(context.data)\n",
    "\n",
    "        # Collect results from the workflow\n",
    "        tool_outputs = []\n",
    "        for func_name, raw_inputs, output_placeholder in func_calls:\n",
    "            if output_placeholder in results:\n",
    "                tool_outputs.append(\n",
    "                    ToolOutput(\n",
    "                        content=results[output_placeholder],\n",
    "                        tool_name=func_name,\n",
    "                        raw_output=results[output_placeholder],\n",
    "                        raw_input={\"args\": raw_inputs},\n",
    "                        is_error=False,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                tool_outputs.append(\n",
    "                    ToolOutput(\n",
    "                        content=\"Error: No output generated\",\n",
    "                        tool_name=func_name,\n",
    "                        raw_output=None,\n",
    "                        raw_input={\"args\": raw_inputs},\n",
    "                        is_error=True,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "        # Replace placeholders in the solution text\n",
    "        for placeholder, value in results.items():\n",
    "            solution = solution.replace(f\"{placeholder}\", '\"' + str(value) + '\"')\n",
    "\n",
    "        return solution, tool_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from pdf_reader_agent.workflow import ConciergeWorkflow as DietConsultantAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "# Define a Pydantic model for input validation\n",
    "class ConsultDietInput(BaseModel):\n",
    "    query: str = Field(description=\"A question or query related to diet.\")\n",
    "\n",
    "# Asynchronous function with Pydantic validation\n",
    "async def consult_diet_async(query: str) -> Dict[str, str]:\n",
    "    concierge = DietConsultantAgent(timeout=180, verbose=True)\n",
    "    result = await concierge.run(query=query)\n",
    "    return result\n",
    "\n",
    "# Synchronous wrapper for the asynchronous function with Pydantic validation\n",
    "def consult_diet(query: str) -> Dict[str, object]:\n",
    "    # Validate the input using the Pydantic model\n",
    "    input_data = ConsultDietInput(query=query)\n",
    "    # Run the asynchronous function\n",
    "    return asyncio.run(consult_diet_async(input_data.query))\n",
    "\n",
    "# Create the FunctionTool with the synchronous wrapper\n",
    "tool = FunctionTool.from_defaults(\n",
    "    fn=consult_diet,\n",
    "    name=\"consult_diet\",\n",
    "    description=\"Consults on diet based on a query.\",\n",
    ")\n",
    "\n",
    "tools_by_name = {\"consult_diet\": tool}\n",
    "\n",
    "\n",
    "\n",
    "# Example solution text for diet consultation\n",
    "solution = \"\"\"1. Retrieve a diet recommendation by querying the diet consultant tool:\n",
    "   - [FUNC consult_diet(\"What is the recommended diet?\") = diet_result]\n",
    "   2. Retrieve a rose information:\n",
    "   - [FUNC consult_diet(\"tell me more abuout rose?\") = rose_result]\n",
    "\"\"\"\n",
    "# Initialize parser and run the workflow\n",
    "# parser = ChainOfAbstractionParser(verbose=True)\n",
    "# solution, tool_outputs = await parser.aparse(solution, tools_by_name)\n",
    "\n",
    "# tool_outputs\n",
    "# print(tool_outputs)\n",
    "# Print the solution and outputs\n",
    "# print(solution)\n",
    "# for output in tool_outputs:\n",
    "#     print(output.content)b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda-llama-index-VIX4Qo5C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c9fee63db1833b128f9e0fcc88438d876a8dd4a4e8fdc61bea76064c0a94131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
