{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_REASONING_PROMPT_TEMPLATE = \"\"\"\n",
    "Generate an initial reasoning step using placeholders for the specific values and function calls needed. \n",
    "The goal is to determine the first function to call based on the question provided and the available functions.\n",
    "\n",
    "Use the placeholders labeled y1, y2, etc., to represent outputs if needed. \n",
    "The reasoning should lead to only one function call, represented as an inline string like [FUNC {{function_name}}({{input1}}, {{input2}}, ...) = {{output_placeholder}}].\n",
    "\n",
    "You are not required to use all functions, but you must use at least one function that best matches the question's intent. \n",
    "If the question can be answered without any function, you can conclude so.\n",
    "\n",
    "Assume someone will read the plan after this function has been executed to continue further steps.\n",
    "{previous_steps} #e.g. {{filtered_retrieval(guidelines for management of frozen shoulder = y1)}}\n",
    "{previous_outcomes} #eg. Evaluator output: retrieved sources are irrelevant and do not answer the question of management of frozen shoulder, the reasoner should execute a new retrieval step. \n",
    "Example: #---->The next step should do a broad article search for froznen shoulder managment using {structured_retrieval(frozen shoulder management=y1)}\n",
    "-----------\n",
    "Available functions:\n",
    "```python\n",
    "def add(a: int, b: int) -> int:\n",
    "    \\\"\\\"\\\"Add two numbers together.\\\"\\\"\\\"\n",
    "    ...\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \\\"\\\"\\\"Multiply two numbers together.\\\"\\\"\\\"\n",
    "    ...\n",
    "```   \n",
    "    \n",
    "Question:\n",
    "Sally has 3 apples and buys 2 more. Then magically, a wizard casts a spell that multiplies the number of apples by 3. How many apples does Sally have now?\n",
    "\n",
    "Abstract plan of reasoning:\n",
    "After buying the apples, Sally has [FUNC add(3, 2) = y1] apples.\n",
    "\n",
    "Your Turn:\n",
    "-----------\n",
    "Available functions:\n",
    "```python\n",
    "{functions}\n",
    "```\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Abstract plan of reasoning:\n",
    "\"\"\"\n",
    "\n",
    "NEXT_STEP_REASONING_PROMPT_TEMPLATE = \"\"\"\n",
    "Based on the outputs generated from the previous function calls, decide whether additional steps are necessary:\n",
    "No further steps are necessary if the retrieved context completely answers the question\n",
    "If no further steps are required, return \"NO\".\n",
    "If the context does not answer the question, return XXXX\n",
    "If further steps are needed, provide a clear reasoning step using placeholders (y1, y2, etc.) for specific values and function calls. The objective is to determine the next function to call, using available outputs and aligning with the intent of the question.\n",
    "{question}\n",
    "\n",
    "{function steps}\n",
    "\n",
    "{outputs}\n",
    "#evaluate how well outputs answer question, if more info needed, trigger another reasoning step with instructions of why more info is pygame.examples.headless_no_windows_needed.main(#e..g, fout, w, h)\n",
    "#e.g. similarity threshold was not highenough so no nodes retrieved, nodes are irrelevant, no guidelines or drug monographs exist, filter too restricve\n",
    "Use placeholders (y1, y2, etc.) to represent the outputs of previous function calls if needed.\n",
    "The reasoning should lead to a single, well-justified function call, represented as: [FUNC {function_name}({input1}, {input2}, ...) = {output_placeholder}].\n",
    "You are not required to use all functions, but you must select at least one function that best matches the question's intent or utilizes keywords from the previous output to refine the results.\n",
    "If the question can be fully answered without invoking additional functions, conclude accordingly.\n",
    "Assumptions:\n",
    "Assume that someone will read this plan after executing the current function to determine the next steps.\n",
    "\n",
    "Example:\n",
    "-----------\n",
    "Available functions:\n",
    "```python\n",
    "def add(a: int, b: int) -> int:\n",
    "    \\\"\\\"\\\"Add two numbers together.\\\"\\\"\\\"\n",
    "    ...\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \\\"\\\"\\\"Multiply two numbers together.\\\"\\\"\\\"\n",
    "    ...\n",
    "```   \n",
    "Question:\n",
    "Sally has 3 apples and buys 2 more. Then magically, a wizard casts a spell that multiplies the number of apples by 3. How many apples does Sally have now?\n",
    "\n",
    "Previous Function Call Output:\n",
    "After buying the apples, Sally has [FUNC add(3, 2) = y1] apples. After executing the first function, we have y1 = 5.\n",
    "\n",
    "Abstract Plan of Reasoning for Next Step:\n",
    "Since a wizard casts a spell that multiplies the number of apples by 3, use the output from the previous function call to determine the total apples. Thus, the next function call is [FUNC multiply(y1, 3)].\n",
    "\n",
    "\n",
    "\n",
    "Your Turn:\n",
    "-----------\n",
    "Available functions:\n",
    "```python\n",
    "{functions}\n",
    "```\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Previous Function Call Output:\n",
    "{function_call_output}\n",
    "\n",
    "Abstract plan of reasoning for next step:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anhthu/Library/Caches/pypoetry/virtualenvs/eda-llama-index-VIX4Qo5C-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/anhthu/Library/Caches/pypoetry/virtualenvs/eda-llama-index-VIX4Qo5C-py3.11/lib/python3.11/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_name\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from agents.coa_agent.prompts.evaluator import prometheus_relevancy_eval_prompt_template, prometheus_relevancy_refine_prompt_template\n",
    "from agents.coa_agent.validator.relevancy_eval import GPT4RelevancyEvaluator\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "relevancy_eval_prompt_template = \"\"\"###Task Description: An instruction (might include an Input inside it), a query, context, and a score rubric representing evaluation criteria are given. \n",
    "       1. You are provided with evaluation task with the help of a query and context output by function tool.\n",
    "       2. Write a detailed feedback based on evaluation task and the given score rubric, not evaluating in general. \n",
    "       3. After writing a feedback, write a score that is YES or NO. You should refer to the score rubric. \n",
    "       4. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (YES or NO)” \n",
    "       5. Please do not generate any other opening, closing, and explanations. \n",
    "\n",
    "        ###The instruction to evaluate: Your task is to evaluate if the source nodes for the query whether they are in line with the context information provided.\n",
    "\n",
    "        ###Query: {query_str} \n",
    "\n",
    "        ###Context: {context_str}\n",
    "            \n",
    "        ###Score Rubrics: \n",
    "        Score YES: If the context information provided is sufficient and in line to answer the query.\n",
    "        Score NO: If the context information provided is sufficient and in line to answer the query.\n",
    "        \n",
    "        ###Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Any\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.base.llms.types import ChatMessage\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "class StepGenerator:\n",
    "    def __init__(self, tools_strs: List[str],  llm: Optional[Any] = None, ):\n",
    "        self.state = \"\"\n",
    "        self.tools_strs = tools_strs  # Store available functions\n",
    "        self.initial_reasoning_template = INITIAL_REASONING_PROMPT_TEMPLATE\n",
    "        self.next_step_reasoning_template = NEXT_STEP_REASONING_PROMPT_TEMPLATE\n",
    "        \n",
    "        if llm is None:\n",
    "            self.llm = OpenAI(temperature=0, model=\"gpt-4o\")\n",
    "        else:\n",
    "            self.llm = llm\n",
    "\n",
    "        \n",
    "    def select_prompt(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Selects the appropriate prompt based on the current state.\n",
    "\n",
    "        Args:\n",
    "            query (str): The question or input for which reasoning is being generated.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted reasoning prompt.\n",
    "        \"\"\"\n",
    "        if not self.state:\n",
    "            # Use the initial reasoning template\n",
    "            reasoning_prompt = self.initial_reasoning_template.format(\n",
    "                functions=\"\\n\".join(self.tools_strs),\n",
    "                question=query\n",
    "            )\n",
    "        else:\n",
    "            # Use the next step reasoning template\n",
    "            reasoning_prompt = self.next_step_reasoning_template.format(\n",
    "                functions=\"\\n\".join(self.tools_strs),\n",
    "                question=query,\n",
    "                function_call_output=self.state\n",
    "            )\n",
    "\n",
    "        return reasoning_prompt\n",
    "    \n",
    "    async def generate_step(self, query: str):\n",
    "        \"\"\"\n",
    "        Generates the reasoning step by selecting the prompt and executing the reasoning.\n",
    "\n",
    "        Args:\n",
    "            gpt4o (OpenAI): The OpenAI instance for processing the reasoning prompt.\n",
    "            query (str): The question or input for which reasoning is being generated.\n",
    "\n",
    "        Returns:\n",
    "            str: The response content from the reasoning step.\n",
    "        \"\"\"\n",
    "        # Select the appropriate reasoning prompt\n",
    "        reasoning_prompt = self.select_prompt(query)\n",
    "        reasoning_message = ChatMessage(role=\"user\", content=reasoning_prompt)\n",
    "\n",
    "        # Run the reasoning prompt\n",
    "        response = await self.llm.achat([reasoning_message])\n",
    "        \n",
    "        solution = response.message.content\n",
    "        \n",
    "        import re\n",
    "        func_calls = re.findall(r\"\\[FUNC (\\w+)\\((.*?)\\) = (\\w+)\\]\", solution)\n",
    "        func_call = func_calls[-1]\n",
    "        \n",
    "        # Update state based on the response content\n",
    "        self.state = solution\n",
    "        return func_call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Tuple\n",
    "from llama_index.core.tools.types import AsyncBaseTool\n",
    "from llama_index.core.workflow import Workflow, Context, Event, StartEvent, StopEvent, step\n",
    "import os\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "class FunctionCallEvent(Event):\n",
    "    func_call: Tuple[str, str, str]  # Function name, raw inputs, output placeholder\n",
    "\n",
    "class ValidateFunctionCallEvent(Event):\n",
    "    input_data: List[Any]\n",
    "    output_placeholder: str\n",
    "    tool_output: Any\n",
    "\n",
    "class InitializeEvent(Event):\n",
    "    \"\"\"Event for initializing the workflow context.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow.py\n",
    "from llama_index.core.workflow import Context, StartEvent, Workflow, StopEvent, step\n",
    "from agents.coa_agent.validator.relevancy_eval import GPT4RelevancyEvaluator\n",
    "from typing import List, Any\n",
    "from agents.coa.tools_handler.tool_retriver import ToolRetriever\n",
    "\n",
    "\n",
    "class ChainOfAbstractionSteps:\n",
    "    def __init__(self):\n",
    "        self.validator = GPT4RelevancyEvaluator(relevancy_eval_prompt_template, prometheus_relevancy_refine_prompt_template)\n",
    "        self.tools_retriever = None\n",
    "        \n",
    "        \n",
    "    @step(pass_context=True)\n",
    "    async def initialize_step(self, ctx: Context, ev: StartEvent | InitializeEvent) -> InitializeEvent | FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Initialize the workflow context.\"\"\"\n",
    "        if isinstance(ev, StartEvent):\n",
    "            if ev.get(\"query\") is None:\n",
    "                return StopEvent(result={\"message\": \"Please provide query\"})\n",
    "            \n",
    "            ctx.data[\"query\"] = ev.get(\"query\")\n",
    "            self.tools_retriever = ToolRetriever(ev.tools)\n",
    "            # Perform initial setup, e.g., load tools, validators, etc.\n",
    "            retrieved_tools = self.tools_retriever.prepare_tools(ctx.data[\"query\"])\n",
    "\n",
    "            ctx.data[\"results\"] = {}\n",
    "            \n",
    "            self.step_generator = StepGenerator(tools_strs=retrieved_tools[\"tools_strs\"] or [])\n",
    "            ctx.data[\"tools_by_name\"] = retrieved_tools[\"tools_by_name\"]\n",
    "            ctx.data[\"function_calls\"] = []\n",
    "            ctx.data[\"iteration\"] = 0\n",
    "            ctx.data[\"accumulated_sources\"] = 0\n",
    "\n",
    "            \n",
    "            return InitializeEvent()\n",
    "\n",
    "        # After initialization, check if there are function calls to process\n",
    "        if ctx.data[\"function_calls\"]:\n",
    "            first_func_call = ctx.data[\"function_calls\"][0]\n",
    "            return FunctionCallEvent(func_call=first_func_call)\n",
    "        else:\n",
    "            # Generate the first reasoning step\n",
    "            first_reasoning = await self.step_generator.generate_step(ctx.data[\"query\"])\n",
    "            if not first_reasoning:\n",
    "                return StopEvent({\"message\": \"No valid reasoning could be generated.\", \"results\": ctx.data[\"results\"]})\n",
    "\n",
    "            return FunctionCallEvent(func_call=first_reasoning)\n",
    "        \n",
    "        \n",
    "            # return StopEvent({\"message\": \"No function calls to process.\", \"results\": ctx.data[\"results\"]})\n",
    "            \n",
    "    @step(pass_context=True)\n",
    "    async def function_call_step(self, ctx: Context, ev: FunctionCallEvent) -> StopEvent:\n",
    "        \"\"\"Execute a function call and prepare for validation.\"\"\"\n",
    "        func_name, raw_inputs, output_placeholder = ev.func_call\n",
    "        input_data = self._prepare_inputs(ctx, raw_inputs)\n",
    "\n",
    "        try:\n",
    "            print(f\"Execute the function {ctx.data['tools_by_name'][func_name]} with inputs: {', '.join(map(str, input_data))}\")\n",
    "            tool_output = await ctx.data[\"tools_by_name\"][func_name].acall(*input_data)\n",
    "            \n",
    "            # print(f\"Executed {func_name} with inputs {input_data} -> {output_placeholder}: {tool_output} \\n\")\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            error_message = f\"Error in {func_name} with inputs {input_data}: {str(e)}\"\n",
    "            print(error_message) \n",
    "            return self._handle_error(error_message)\n",
    "\n",
    "        \n",
    "        # if self.validator:\n",
    "        #     return ValidateFunctionCallEvent(\n",
    "        #         input_data=input_data + [tool_output.raw_output],\n",
    "        #         output_placeholder=output_placeholder,\n",
    "        #         tool_output=tool_output\n",
    "        #     )\n",
    "\n",
    "\n",
    "        ctx.data[\"results\"][output_placeholder] = tool_output.content\n",
    "        return StopEvent({\"message\": \"No function calls to process.\", \"results\": ctx.data[\"results\"]})\n",
    "        # return await self._move_to_next_function(ctx)\n",
    "\n",
    "    def _prepare_inputs(self, ctx: Context, raw_inputs: str) -> List[Any]:\n",
    "        \"\"\"Parse and prepare function inputs.\"\"\"\n",
    "        results = ctx.data[\"results\"]\n",
    "        input_data = []\n",
    "        for raw_input in raw_inputs.split(\",\"):\n",
    "            raw_input = raw_input.strip()\n",
    "            try:\n",
    "                input_data.append(int(results.get(raw_input, raw_input)))\n",
    "            except ValueError:\n",
    "                input_data.append(raw_input)  # Handle non-integer inputs\n",
    "        return input_data\n",
    "    \n",
    "\n",
    "    def _handle_error(self, message: str) -> StopEvent:\n",
    "        \"\"\"Handle errors gracefully and provide feedback.\"\"\"\n",
    "        return StopEvent({\"message\": message, \"results\": {}})\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfAbstractionWorkflow(Workflow):\n",
    "\n",
    "    steps = ChainOfAbstractionSteps()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def initialize_step(self, ctx: Context, ev: StartEvent | InitializeEvent) -> InitializeEvent | FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Initialize the workflow context and decide initial reasoning.\"\"\"\n",
    "        return await self.steps.initialize_step(ctx, ev)\n",
    "    \n",
    "    @step(pass_context=True)\n",
    "    async def function_call_step(self, ctx: Context, ev: FunctionCallEvent) -> StopEvent:\n",
    "        return await self.steps.function_call_step(ctx, ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import asyncio\n",
    "from typing import Dict\n",
    "from pydantic import BaseModel, Field\n",
    "from agents.pdf_reader_agent.workflow import ConciergeWorkflow as DietConsultantAgent, ConciergeWorkflow as NutritionConsultantAgent\n",
    "\n",
    "\n",
    "class DietQuery(BaseModel):\n",
    "    query: str = Field(description=\"A question or query related to diet.\")\n",
    "\n",
    "async def consult_diet_async(query: str) -> Dict[str, str]:\n",
    "    concierge = DietConsultantAgent(timeout=180, verbose=True)\n",
    "    result = await concierge.run(query=query, collection_name=\"pdf-diet-docs\")\n",
    "    return result\n",
    "\n",
    "def consult_diet(query: str) -> Dict[str, object]:\n",
    "    input_data = DietQuery(query=query)\n",
    "    # Run the asynchronous function\n",
    "    return asyncio.run(consult_diet_async(input_data.query))\n",
    "\n",
    "\n",
    "class NutritionQuery(BaseModel):\n",
    "    query: str = Field(description=\"A question or query related to nutrition.\")\n",
    "\n",
    "async def consult_nutrition_async(query: str) -> Dict[str, str]:\n",
    "    concierge = NutritionConsultantAgent(timeout=180, verbose=True)\n",
    "    result = await concierge.run(query=query, collection_name=\"pdf-nutrition-docs\")\n",
    "    return result\n",
    "\n",
    "def consult_nutrition(query: str) -> Dict[str, object]:\n",
    "    # Validate the input using the updated Pydantic model\n",
    "    input_data = NutritionQuery(query=query)\n",
    "    # Run the asynchronous function\n",
    "    return asyncio.run(consult_nutrition_async(input_data.query))\n",
    "\n",
    "\n",
    "diet_tool = FunctionTool.from_defaults(\n",
    "        fn=consult_diet,\n",
    "        name=\"consult_diet\",\n",
    "        description=\"Consults on diet based on a query.\",\n",
    "    )\n",
    "\n",
    "nutrition_tool = FunctionTool.from_defaults(\n",
    "    fn=consult_nutrition,\n",
    "    name=\"consult_nutrition\",\n",
    "    description=\"Provides nutritional information based on a query.\",\n",
    ")\n",
    "\n",
    "tools = [diet_tool, nutrition_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('consult_diet', '\"Indo-Mediterranean diet\"', 'y1')\n",
      "Execute the function <llama_index.core.tools.function_tool.FunctionTool object at 0x29e4508d0> with inputs: \"Indo-Mediterranean diet\"\n",
      "Running step concierge\n",
      "Step concierge produced event InitializeEvent\n",
      "Running step initialize\n",
      "Step initialize produced event ConciergeEvent\n",
      "Running step concierge\n",
      "Step concierge produced event OrchestratorEvent\n",
      "Running step orchestrator\n",
      "Step orchestrator produced event QueryEvent\n",
      "Running step query_index\n",
      "Step query_index produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'message': 'No function calls to process.',\n",
       " 'results': {'y1': '{\\'query_result\\': \\'The \"Indo-Mediterranean diet\" is a dietary pattern used in a randomized trial involving Indian patients with pre-existing coronary heart disease or high cardiovascular risk. This diet is rich in whole grains, fruits, vegetables, walnuts, almonds, and oils such as mustard or soybean oil, which are high in alpha-linolenic acid. Compared to a control group following a step I National Cholesterol Education Program (NCEP) diet, patients on the \"Indo-Mediterranean diet\" experienced approximately a 60% reduction in the rate of cardiovascular death and about a 50% reduction in the risk of non-fatal myocardial infarction.\\', \\'source_node\\': [\\'Int. J. Environ. Res. Public Health 2019, 16, 942 6 of 16 (the Mediterranean Diet Adherence Screener, or MeDiet score) and was found to be inversely associated with the rate of cardiovascular events [36]. Other analyses on the population of the PREDIMED study further showed that the Mediterranean diet seemed to reduce the expression of pro-atherogenic genes [37], cardiovascular risk surrogate markers such as waist-to-hip ratio, lipid fractions, lipoprotein particles, oxidative stress, and markers of inflammation [38,39], but also the risk of developing metabolic syndrome [40] and type 2 diabetes [41]. Nevertheless, the initial results in PREDIMED study were challenged on the account of the small rates of cardiovascular events in all three intervention groups, which could have induced the aforementioned statistically significant differences on the basis of an imperfect randomization procedure, allowing a few biases in the baseline groups’ characteristics [42,43]. The authors chose to retract the first publication [35] and to reanalyze the data after exclusion of sites with randomization deviations; final results still showed significant reductions in the rate of cardiovascular events (31% in the group following a supplementation with extra-virgin olive oil and 28% in the group following a supplementation with mixed nuts) [44]. Attempts to adapt to the Mediterranean eating style and search for related cardiovascular benefits exist today far beyond the borders of the Mediterranean region. Indian patients with pre-existing coronary heart disease or with high cardiovascular risk were included in another randomized trial using a so-called “Indo-Mediterranean diet” rich in whole grains, fruits, vegetables, walnuts, almonds, mustard or soybean oil, all bringing a high content of alpha-linolenic acid, and compared to a control group randomized to a step I National Cholesterol Education Program (NCEP) diet. Patients following the “Indo-Mediterranean diet” had an approximately 60% reduction in the rate of cardiovascular death and an approximately 50% reduction in the risk for non-fatal myocardial infarction [45].\\', \\'Int. J. Environ. Res. Public Health 2019, 16, 942 3 of 16 Plants originating millet, sorghum, artichokes, okra, watermelons, melons from Africa corn, other beans, peanuts, tomatoes, peppers, eggplant, squash, zucchini, potato, Plants originating sweet potato, prickly pears, cashews, sunflower seeds, avocado, coffee, chocolate, from the Americas cayenne pepper, allspice, pink pepper The food patterns on the shores of the Mediterranean Sea were largely influenced by the three main monotheistic faiths succeeding in this area: Judaism, Christianity, and Islam [7]. These religions also adopted, maintained alive, and held as sacred some of the essential components of the Mediterranean lifestyle [11]. The Mediterranean diet is not, in fact, a unique diet in today’s meaning of the word “diet”. Each of the regions in the Mediterranean Basin developed its own recipes, preferences, and restrictions. The term “Mediterranean diet” could be best understood as a peculiar “dietary pattern” featuring an inter-related set of specific characteristics. Descriptions including only some foods present in the popular culture, while ignoring the absence of other traditional foods or allowing the addition of foods belonging to other eating cultures and patterns should not be accepted as legitimate versions of the Mediterranean diet [14]. An authentic Mediterranean diet pattern should be seen “as a whole”, comprising all its features and not just a part of them [15]. First, olive oil plays a central role in the cooking process, and thus, represents the main source of dietary fat. Cheese is used in limited servings and usually within salads. Meat, milk, and eggs are consumed with a low frequency and in small amounts, and processed meat and sweets are practically non-existent. The Mediterranean diet hence represents, in fact, the only traditional dietary pattern where consumption of saturated and trans fats is inherently minimal. Second, olive oil consumption is associated with a higher vegetable intake, cooked as salads, and to an equally high legume intake in thermic-prepared foods, meaning the Mediterranean diet is essentially a plant-based dietary pattern. Other key components of the Mediterranean diet are the whole grains, nuts, fresh fruits, and a moderate fish intake.\\']}'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = ChainOfAbstractionWorkflow(timeout=100)\n",
    "await w.run(query=\"Tell me more about Indo-mediterranean diet\", tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@step(pass_context=True)\n",
    "    async def function_call_step(self, ctx: Context, ev: FunctionCallEvent) -> ValidateFunctionCallEvent | StopEvent:\n",
    "        \"\"\"Execute a function call and prepare for validation.\"\"\"\n",
    "        func_name, raw_inputs, output_placeholder = ev.func_call\n",
    "        input_data = self._prepare_inputs(ctx, raw_inputs)\n",
    "\n",
    "        try:\n",
    "            tool_output = await ctx.data[\"tools_by_name\"][func_name].acall(*input_data)\n",
    "            \n",
    "            # print(f\"Executed {func_name} with inputs {input_data} -> {output_placeholder}: {tool_output} \\n\")\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error in {func_name} with inputs {input_data}: {str(e)}\"\n",
    "            print(error_message)  # Log error for debugging\n",
    "            return self._handle_error(error_message)\n",
    "\n",
    "        \n",
    "        if self.validator:\n",
    "            return ValidateFunctionCallEvent(\n",
    "                input_data=input_data + [tool_output.raw_output],\n",
    "                output_placeholder=output_placeholder,\n",
    "                tool_output=tool_output\n",
    "            )\n",
    "\n",
    "\n",
    "        ctx.data[\"results\"][output_placeholder] = tool_output.content\n",
    "        return await self._move_to_next_function(ctx)\n",
    "\n",
    "    def _prepare_inputs(self, ctx: Context, raw_inputs: str) -> List[Any]:\n",
    "        \"\"\"Parse and prepare function inputs.\"\"\"\n",
    "        results = ctx.data[\"results\"]\n",
    "        input_data = []\n",
    "        for raw_input in raw_inputs.split(\",\"):\n",
    "            raw_input = raw_input.strip()\n",
    "            try:\n",
    "                input_data.append(int(results.get(raw_input, raw_input)))\n",
    "            except ValueError:\n",
    "                input_data.append(raw_input)  # Handle non-integer inputs gracefully\n",
    "        return input_data\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def validate_function_step(self, ctx: Context, ev: ValidateFunctionCallEvent) -> FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Validate function output and decide on the next step.\"\"\"\n",
    "        try:\n",
    "            validator_output = self.validator.evaluate_sources(*ev.input_data)\n",
    "            # print(f\"Validation result: {validator_output} | Tool output: {ev.tool_output}\")\n",
    "        except Exception as e:\n",
    "            return self._handle_error(f\"Validation error: {e}\")\n",
    "\n",
    "        if validator_output:\n",
    "            \n",
    "            source_nodes, length_nodes = validator_output\n",
    "            \n",
    "            if length_nodes > 0:\n",
    "                ctx.data[\"results\"][ev.output_placeholder] = str(source_nodes)\n",
    "                # Accumulate source if validation is successful\n",
    "                ctx.data[\"accumulated_sources\"] += length_nodes\n",
    "\n",
    "                # Check if the maximum number of sources has been accumulated\n",
    "                if ctx.data[\"accumulated_sources\"] >= self.max_num_sources:\n",
    "                    return StopEvent({\"message\": \"Maximum number of sources accumulated.\", \"results\": ctx.data[\"results\"]})\n",
    "\n",
    "            return await self._move_to_next_function(ctx)\n",
    "\n",
    "        return self._handle_error(\"Tool output does not match the expected validation result\")\n",
    "\n",
    "    async def _move_to_next_function(self, ctx: Context) -> FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Move to the next function call if available.\"\"\"\n",
    "        iteration = ctx.data[\"iteration\"]\n",
    "        function_calls = ctx.data[\"function_calls\"]\n",
    "\n",
    "        if iteration + 1 < len(function_calls):\n",
    "            ctx.data[\"iteration\"] += 1\n",
    "            next_func_call = function_calls[ctx.data[\"iteration\"]]\n",
    "            return FunctionCallEvent(func_call=next_func_call)\n",
    "\n",
    "        # End of the workflow, produce a StopEvent\n",
    "        return StopEvent({\"message\": \"All function calls processed.\", \"results\": ctx.data[\"results\"]})\n",
    "\n",
    "    def _handle_error(self, message: str) -> StopEvent:\n",
    "        \"\"\"Handle errors gracefully and provide feedback.\"\"\"\n",
    "        return StopEvent({\"message\": message, \"results\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfAbstractionWorkflow(Workflow):\n",
    "\n",
    "    steps = ChainOfAbstractionSteps()\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.step_generator = StepGenerator()\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def initialize_step(self, ctx: Context, ev: StartEvent | InitializeEvent) -> InitializeEvent | FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Initialize the workflow context and decide initial reasoning.\"\"\"\n",
    "        return await self.steps.initialize_step(ctx, ev)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def initial_reasoning_step(self, ctx: Context, ev: FunctionCallEvent) -> FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Generate initial reasoning to decide the first function call.\"\"\"\n",
    "        return await self.steps.initial_reasoning_step(ctx, ev)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def function_call_step(self, ctx: Context, ev: FunctionCallEvent) -> ValidateFunctionCallEvent | StopEvent:\n",
    "        \"\"\"Execute the function based on the current plan.\"\"\"\n",
    "        return await self.steps.function_call_step(ctx, ev)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def validate_function_step(self, ctx: Context, ev: ValidateFunctionCallEvent) -> EvaluateFunctionEvent | StopEvent:\n",
    "        \"\"\"Validate the function output and prepare for evaluation.\"\"\"\n",
    "        return await self.steps.validate_function_step(ctx, ev)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def evaluate_function_step(self, ctx: Context, ev: EvaluateFunctionEvent) -> StepGeneratorEvent | StopEvent:\n",
    "        \"\"\"Evaluate the validated function output to decide the next step.\"\"\"\n",
    "        evaluation_result = await self.steps.evaluate_function_step(ctx, ev)\n",
    "        return StepGeneratorEvent(evaluation_result=evaluation_result)\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def step_generator(self, ctx: Context, ev: StepGeneratorEvent) -> FunctionCallEvent | StopEvent:\n",
    "        \"\"\"Generate the next step based on the evaluation result.\"\"\"\n",
    "        return await self.step_generator.generate_next_steps(ctx, ev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda-llama-index-VIX4Qo5C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c9fee63db1833b128f9e0fcc88438d876a8dd4a4e8fdc61bea76064c0a94131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
