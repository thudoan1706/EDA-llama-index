{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    Event,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JokeEvent(Event):\n",
    "    joke: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis:\n",
      "This joke plays on the pun of \"fish and ships\" sounding like \"fish and chips,\" a popular dish at seafood restaurants. The joke also incorporates the pirate theme by mentioning a pirate going to a seafood restaurant, which adds an element of humor and surprise.\n",
      "\n",
      "Critique:\n",
      "Overall, this joke is light-hearted and playful, making it suitable for a general audience. The use of wordplay is clever and adds an element of wit to the punchline. However, the joke may be considered somewhat predictable as the punchline is somewhat obvious once the setup is given. Additionally, the humor may not appeal to everyone as it relies on a specific pun that may not resonate with all listeners. Overall, while the joke is amusing and well-crafted, it may not be considered particularly original or groundbreaking.\n"
     ]
    }
   ],
   "source": [
    "class JokeFlow(Workflow):\n",
    "    llm = OpenAI()\n",
    "\n",
    "    @step()\n",
    "    async def generate_joke(self, ev: StartEvent) -> JokeEvent:\n",
    "        topic = ev.topic\n",
    "        prompt = f\"Write your best joke about {topic}.\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return JokeEvent(joke=str(response))\n",
    "\n",
    "    @step()\n",
    "    async def critique_joke(self, ev: JokeEvent) -> StopEvent:\n",
    "        joke = ev.joke\n",
    "\n",
    "        prompt = f\"Give a thorough analysis and critique of the following joke: {joke}\"\n",
    "        response = await self.llm.acomplete(prompt)\n",
    "        return StopEvent(result=str(response))\n",
    "\n",
    "\n",
    "w = JokeFlow(timeout=60, verbose=False)\n",
    "result = await w.run(topic=\"pirates\")\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke_flow_all.html\n",
      "joke_flow_recent.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n",
    "\n",
    "# Draw all\n",
    "draw_all_possible_flows(JokeFlow, filename=\"joke_flow_all.html\")\n",
    "\n",
    "# Draw an execution\n",
    "w = JokeFlow()\n",
    "await w.run(topic=\"Pirates\")\n",
    "draw_most_recent_execution(w, filename=\"joke_flow_recent.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MyEventResult(result='result'), MyEventResult(result='result')]\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "from llama_index.core.workflow import step, Context, Event, Workflow, StartEvent, StopEvent\n",
    "\n",
    "class MyEvent(Event):\n",
    "    pass\n",
    "\n",
    "class MyEventResult(Event):\n",
    "    result: str\n",
    "\n",
    "class GatherEvent(Event):\n",
    "    pass\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step()\n",
    "    async def dispatch_step(self, ev: StartEvent) -> Union[MyEvent, GatherEvent]:\n",
    "        # Manually trigger two MyEvent events\n",
    "        self.send_event(MyEvent())\n",
    "        self.send_event(MyEvent())\n",
    "        \n",
    "        # Return a GatherEvent to signal the next step\n",
    "        return GatherEvent()\n",
    "\n",
    "    @step()\n",
    "    async def handle_my_event(self, ev: MyEvent) -> MyEventResult:\n",
    "        # Handle MyEvent and return a MyEventResult\n",
    "        return MyEventResult(result=\"result\")\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def gather(self, ctx: Context, ev: Union[GatherEvent, MyEventResult]) -> Union[StopEvent, None]:\n",
    "        # Wait for MyEventResult events to finish\n",
    "        events = ctx.collect_events(ev, [MyEventResult, MyEventResult])\n",
    "        if not events:\n",
    "            return None\n",
    "        \n",
    "        # Return a StopEvent with the gathered results\n",
    "        return StopEvent(result=events)\n",
    "\n",
    "# Running the workflow in an async environment\n",
    "async def main():\n",
    "    workflow = MyWorkflow()\n",
    "    \n",
    "    # Pass any necessary attributes directly to the run() method\n",
    "    result = await workflow.run(topic=\"example_topic\")  # Use keyword arguments here\n",
    "    print(result)\n",
    "\n",
    "# Use await main() in interactive environments like Jupyter Notebook\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_flow_all.html\n",
      "my_flow_recent.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n",
    "\n",
    "# Assuming JokeFlow is already defined as per your previous examples\n",
    "# Draw all possible flows\n",
    "draw_all_possible_flows(MyWorkflow, filename=\"my_flow_all.html\")\n",
    "\n",
    "# Draw an execution\n",
    "w = MyWorkflow()\n",
    "await w.run(topic=\"Pirates\")\n",
    "draw_most_recent_execution(w, filename=\"my_flow_recent.html\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection Workflow for Structured Outputs¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class ExtractionDone(Event):\n",
    "    output: str\n",
    "    passage: str\n",
    "\n",
    "\n",
    "class ValidationErrorEvent(Event):\n",
    "    error: str\n",
    "    wrong_output: str\n",
    "    passage: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Car(BaseModel):\n",
    "    brand: str\n",
    "    model: str\n",
    "    power: int\n",
    "\n",
    "\n",
    "class CarCollection(BaseModel):\n",
    "    cars: list[Car]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from llama_index.core.workflow import (\n",
    "    Workflow,\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Context,\n",
    "    step,\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "EXTRACTION_PROMPT = \"\"\"\n",
    "Context information is below:\n",
    "---------------------\n",
    "{passage}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge, create a JSON object from the information in the context.\n",
    "The JSON object must follow the JSON schema:\n",
    "{schema}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"\n",
    "You already created this output previously:\n",
    "---------------------\n",
    "{wrong_answer}\n",
    "---------------------\n",
    "\n",
    "This caused the JSON decode error: {error}\n",
    "\n",
    "Try again, the response must contain only valid JSON code. Do not add any sentence before or after the JSON object.\n",
    "Do not repeat the schema.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ReflectionWorkflow(Workflow):\n",
    "    max_retries: int = 3\n",
    "\n",
    "    @step(pass_context=True)\n",
    "    async def extract(\n",
    "        self, ctx: Context, ev: StartEvent | ValidationErrorEvent\n",
    "    ) -> StopEvent | ExtractionDone:\n",
    "        current_retries = ctx.data.get(\"retries\", 0)\n",
    "        if current_retries >= self.max_retries:\n",
    "            return StopEvent(result=\"Max retries reached\")\n",
    "        else:\n",
    "            ctx.data[\"retries\"] = current_retries + 1\n",
    "\n",
    "        if isinstance(ev, StartEvent):\n",
    "            passage = ev.get(\"passage\")\n",
    "            if not passage:\n",
    "                return StopEvent(result=\"Please provide some text in input\")\n",
    "            reflection_prompt = \"\"\n",
    "        elif isinstance(ev, ValidationErrorEvent):\n",
    "            passage = ev.passage\n",
    "            reflection_prompt = REFLECTION_PROMPT.format(\n",
    "                wrong_answer=ev.wrong_output, error=ev.error\n",
    "            )\n",
    "\n",
    "        llm = Ollama(model=\"llama3.1\", request_timeout=120)\n",
    "        prompt = EXTRACTION_PROMPT.format(\n",
    "            passage=passage, schema=CarCollection.schema_json()\n",
    "        )\n",
    "        if reflection_prompt:\n",
    "            prompt += reflection_prompt\n",
    "            \n",
    "        output = await llm.acomplete(prompt)\n",
    "\n",
    "        return ExtractionDone(output=str(output), passage=passage)\n",
    "\n",
    "    @step()\n",
    "    async def validate(\n",
    "        self, ev: ExtractionDone\n",
    "    ) -> StopEvent | ValidationErrorEvent:\n",
    "        try:\n",
    "            json.loads(ev.output)\n",
    "        except Exception as e:\n",
    "            print(\"Validation failed, retrying...\")\n",
    "            \n",
    "            return ValidationErrorEvent(\n",
    "                error=str(e), wrong_output=ev.output, passage=ev.passage\n",
    "            )\n",
    "\n",
    "        return StopEvent(result=ev.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step extract\n",
      "Step extract produced event ExtractionDone\n",
      "Running step validate\n",
      "Here is the JSON object based on the provided context information:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"$defs\": {\n",
      "    \"Car\": {\n",
      "      \"properties\": {\n",
      "        \"brand\": {\"title\": \"Brand\", \"type\": \"string\"},\n",
      "        \"model\": {\"title\": \"Model\", \"type\": \"string\"},\n",
      "        \"power\": {\"title\": \"Power\", \"type\": \"integer\"}\n",
      "      },\n",
      "      \"required\": [\"brand\", \"model\", \"power\"],\n",
      "      \"title\": \"Car\",\n",
      "      \"type\": \"object\"\n",
      "    }\n",
      "  },\n",
      "  \"properties\": {\n",
      "    \"cars\": {\n",
      "      \"items\": {\n",
      "        \"$ref\": \"#/$defs/Car\"\n",
      "      },\n",
      "      \"title\": \"Cars\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\"cars\"],\n",
      "  \"title\": \"CarCollection\",\n",
      "  \"type\": \"object\",\n",
      "  \"cars\": [\n",
      "    {\n",
      "      \"brand\": \"Fiat\",\n",
      "      \"model\": \"Panda\",\n",
      "      \"power\": 45\n",
      "    },\n",
      "    {\n",
      "      \"brand\": \"Honda\",\n",
      "      \"model\": \"Civic\",\n",
      "      \"power\": 30\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "This JSON object follows the provided schema and includes the two cars mentioned in the context, with their respective brands, models, and powers.\n",
      "Invalid JSON output has been written to invalid_output.json\n",
      "Validation failed, retrying...\n",
      "Step validate produced event ValidationErrorEvent\n",
      "Running step extract\n",
      "Step extract produced event ExtractionDone\n",
      "Running step validate\n",
      "{\n",
      "  \"$defs\": {\n",
      "    \"Car\": {\n",
      "      \"properties\": {\n",
      "        \"brand\": {\"title\": \"Brand\", \"type\": \"string\"},\n",
      "        \"model\": {\"title\": \"Model\", \"type\": \"string\"},\n",
      "        \"power\": {\"title\": \"Power\", \"type\": \"integer\"}\n",
      "      },\n",
      "      \"required\": [\"brand\", \"model\", \"power\"],\n",
      "      \"title\": \"Car\",\n",
      "      \"type\": \"object\"\n",
      "    }\n",
      "  },\n",
      "  \"properties\": {\n",
      "    \"cars\": {\n",
      "      \"items\": {\n",
      "        \"$ref\": \"#/$defs/Car\"\n",
      "      },\n",
      "      \"title\": \"Cars\",\n",
      "      \"type\": \"array\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\"cars\"],\n",
      "  \"title\": \"CarCollection\",\n",
      "  \"type\": \"object\",\n",
      "  \"cars\": [\n",
      "    {\n",
      "      \"brand\": \"Fiat\",\n",
      "      \"model\": \"Panda\",\n",
      "      \"power\": 45\n",
      "    },\n",
      "    {\n",
      "      \"brand\": \"Honda\",\n",
      "      \"model\": \"Civic\",\n",
      "      \"power\": 30\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "Invalid JSON output has been written to invalid_output.json\n",
      "Step validate produced event StopEvent\n"
     ]
    }
   ],
   "source": [
    "w = ReflectionWorkflow(timeout=120, verbose=True)\n",
    "\n",
    "# Run the workflow\n",
    "ret = await w.run(\n",
    "    passage=\"I own two cars: a Fiat Panda with 45Hp and a Honda Civic with 30Hp.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data has been written to output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming `ret` is your raw JSON string from the workflow output\n",
    "raw_json = ret\n",
    "\n",
    "# Parse the JSON string into a Python dictionary\n",
    "parsed_json = json.loads(raw_json)\n",
    "\n",
    "# Define the filename where you want to save the JSON data\n",
    "filename = 'output.json'\n",
    "\n",
    "# Dump the formatted JSON into a file\n",
    "with open(filename, 'w') as json_file:\n",
    "    json.dump(parsed_json, json_file, indent=4)\n",
    "\n",
    "print(f\"JSON data has been written to {filename}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow for a Function Calling Agent¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import ToolSelection, ToolOutput\n",
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class InputEvent(Event):\n",
    "    input: list[ChatMessage]\n",
    "\n",
    "\n",
    "class ToolCallEvent(Event):\n",
    "    tool_calls: list[ToolSelection]\n",
    "\n",
    "\n",
    "class FunctionOutputEvent(Event):\n",
    "    output: ToolOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.core.llms.function_calling import FunctionCallingLLM\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools.types import BaseTool\n",
    "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "\n",
    "class FuncationCallingAgent(Workflow):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        llm: FunctionCallingLLM | None = None,\n",
    "        tools: List[BaseTool] | None = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tools = tools or []\n",
    "\n",
    "        self.llm = llm or OpenAI()\n",
    "        assert self.llm.metadata.is_function_calling_model\n",
    "\n",
    "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
    "        self.sources = []\n",
    "\n",
    "    @step()\n",
    "    async def prepare_chat_history(self, ev: StartEvent) -> InputEvent:\n",
    "        # clear sources\n",
    "        self.sources = []\n",
    "\n",
    "        # get user input\n",
    "        user_input = ev.input\n",
    "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
    "        self.memory.put(user_msg)\n",
    "\n",
    "        # get chat history\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)\n",
    "\n",
    "    @step()\n",
    "    async def handle_llm_input(\n",
    "        self, ev: InputEvent\n",
    "    ) -> ToolCallEvent | StopEvent:\n",
    "        chat_history = ev.input\n",
    "\n",
    "        response = await self.llm.achat_with_tools(\n",
    "            self.tools, chat_history=chat_history\n",
    "        )\n",
    "        self.memory.put(response.message)\n",
    "\n",
    "        tool_calls = self.llm.get_tool_calls_from_response(\n",
    "            response, error_on_no_tool_call=False\n",
    "        )\n",
    "\n",
    "        if not tool_calls:\n",
    "            return StopEvent(\n",
    "                result={\"response\": response, \"sources\": [*self.sources]}\n",
    "            )\n",
    "        else:\n",
    "            return ToolCallEvent(tool_calls=tool_calls)\n",
    "\n",
    "    @step()\n",
    "    async def handle_tool_calls(self, ev: ToolCallEvent) -> InputEvent:\n",
    "        tool_calls = ev.tool_calls\n",
    "        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}\n",
    "\n",
    "        tool_msgs = []\n",
    "\n",
    "        # call tools -- safely!\n",
    "        for tool_call in tool_calls:\n",
    "            tool = tools_by_name.get(tool_call.tool_name)\n",
    "            additional_kwargs = {\n",
    "                \"tool_call_id\": tool_call.tool_id,\n",
    "                \"name\": tool.metadata.get_name(),\n",
    "            }\n",
    "            if not tool:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Tool {tool_call.tool_name} does not exist\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                tool_output = tool(**tool_call.tool_kwargs)\n",
    "                self.sources.append(tool_output)\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=tool_output.content,\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                tool_msgs.append(\n",
    "                    ChatMessage(\n",
    "                        role=\"tool\",\n",
    "                        content=f\"Encountered error in tool call: {e}\",\n",
    "                        additional_kwargs=additional_kwargs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for msg in tool_msgs:\n",
    "            self.memory.put(msg)\n",
    "\n",
    "        chat_history = self.memory.get()\n",
    "        return InputEvent(input=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = FuncationCallingAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step prepare_chat_history\n",
      "Step prepare_chat_history produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "SOURCE [ToolOutput(content='20', tool_name='multiply', raw_input={'args': (), 'kwargs': {'x': 4, 'y': 5}}, raw_output=20, is_error=False)]\n",
      "Step handle_tool_calls produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event ToolCallEvent\n",
      "Running step handle_tool_calls\n",
      "SOURCE [ToolOutput(content='20', tool_name='multiply', raw_input={'args': (), 'kwargs': {'x': 4, 'y': 5}}, raw_output=20, is_error=False), ToolOutput(content='30', tool_name='add', raw_input={'args': (), 'kwargs': {'x': 20, 'y': 10}}, raw_output=30, is_error=False)]\n",
      "Step handle_tool_calls produced event InputEvent\n",
      "Running step handle_llm_input\n",
      "Step handle_llm_input produced event StopEvent\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Useful function to add two numbers.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def multiply(x: int, y: int) -> int:\n",
    "    \"\"\"Useful function to multiply two numbers.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "\n",
    "tools = [\n",
    "    FunctionTool.from_defaults(add),\n",
    "    FunctionTool.from_defaults(multiply),\n",
    "]\n",
    "\n",
    "agent = FuncationCallingAgent(\n",
    "    llm=OpenAI(model=\"gpt-4o\"), tools=tools, timeout=120, verbose=True\n",
    ")\n",
    "\n",
    "ret = await agent.run(input=\"What is (4 x 5) + 10!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The result of \\\\((4 \\\\times 5) + 10\\\\) is \\\\(30\\\\).', additional_kwargs={}), raw=ChatCompletion(id='chatcmpl-9vsTzdexzzq2O0nFYeE1Qub87hYKu', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The result of \\\\((4 \\\\times 5) + 10\\\\) is \\\\(30\\\\).', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1723581155, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_3aa7262c27', usage=CompletionUsage(completion_tokens=22, prompt_tokens=169, total_tokens=191)), delta=None, logprobs=None, additional_kwargs={}),\n",
       " 'sources': [ToolOutput(content='20', tool_name='multiply', raw_input={'args': (), 'kwargs': {'x': 4, 'y': 5}}, raw_output=20, is_error=False),\n",
       "  ToolOutput(content='30', tool_name='add', raw_input={'args': (), 'kwargs': {'x': 20, 'y': 10}}, raw_output=30, is_error=False)]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda-llama-index-VIX4Qo5C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c9fee63db1833b128f9e0fcc88438d876a8dd4a4e8fdc61bea76064c0a94131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
